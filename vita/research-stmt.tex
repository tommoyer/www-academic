\clearpage
\hypertarget{research-statement}{%
\section{Research Statement}\label{research-statement}}


I have spent the majority of my research career focused on problems related to ensuring the integrity of systems and the data they process. This started back in graduate school, and has continued through today. Over the years, the focus has shifted slightly as the problem space changes, and new technologies are developed that address research challenges. The first area that I considered in graduate school examined ways to prove the integrity of a server that is hosting data. Later in my graduate career, the focus shifted to include a rudimentary form of data provenance (history of data), tracking the history of rows in a database. As I transitioned from being a graduate student to a researcher at MIT Lincoln Laboratory, many of these same challenges emerged, and I continued to study the ideas necessary to prove that a system is in a known good state. As I started to examine these challenges in a real-world setting for the Department of Defense, I learned a valuable lesson. Namely, that software is relatively simple to protect, but the data that often drives the most critical decisions is much harder to protect. This has become the central focus of my research over the past several years, and will continue to be an area where I focus my efforts in the future.

My focus on system integrity began with a very simple sounding question. ``How do I know that the web site I connect to that shows me all of the correct security indicators, such as a green padlock icon in the address bar, is running the correct software?'' As it turns out, this is a challenging question, and one that is even more challenging when you have the dual goal of also ensuring that the web server can continue to provide a reasonable throughput. This question began with an exploration of trusted hardware that can be used to ``measure'' the state of a system. Here, the state is constrained to static data and software running on the system. Ultimately, I was able to build a system that could serve thousands of requests per second while providing strong guarantees about the data and software on the system. This system was then expanded to handle dynamic content, and later to include rudimentary provenance, all while maintaining acceptable performance. 

This work transitioned to a real-world deployment for the Department of Defense as I transitioned from a graduate student at Penn State to a research scientist at MIT Lincoln Laboratory. The core ideas of proving system integrity were integrated into a prototype system for the United States Army. As this prototype was being developed, my colleagues and I quickly realized that while the software was static, and thus simpler to validate, the data that drove many of the critical decisions was not static, and was often processed by other systems before being used by our prototype. This led to the natural extension of the questions we had been asking, namely, ``How can we validate the data being used in critical decision processes?''

Ultimately, this led to the use of data provenance within these systems. Initial efforts focused on gathering provenance from and validating the execution simple data processing pipelines. As this line of questioning developed, again the question came up ``How can we prove that this is actually true in the face of an adversary that controls the system?'' To answer that question, a summer intern and I developed a new framework within the operating system that provides strong guarantees about the integrity of the provenance data being collected. As this framework was deployed, it has led to a number of other research challenges and opportunities. Two of the major challenges that I will continue to focus on in the near future are the sheer volume of data being generated\footnote{On the order of one DVD worth of data every 10 minutes when the system is fully loaded.} and the lack of context in the gathered data. As a concrete example, consider a database that stores data in rows and columns. This data has structure that is important in understanding how critical the data is. However, from the operating system's perspective, the database is reading a writing to random blocks on disk, a problem known as a \emph{semantic gap}.

While the problems in data provenance are interesting, they will not have direct real-world impact. Instead, my focus has shifted slightly from ``How can we build trustworthy and efficient provenance systems?'' to ``How can we leverage provenance to build systems that can respond to an attacker without significant human intervention?'' I liken this to the idea of the immune system in the human body. The fundamental question I am trying to answer is ``Now that the system has been compromised, what can be done to evict the attacker, restore the system and data, and evolve to prevent future attacks?'' Building such resilient systems will have tremendous impact as we continue to hear about more and more systems being compromised and sensitive data being leaked at every turn.